# Interview System Diagnostic Report
**Date:** 2025-12-14
**Session Analyzed:** `session_20251213_232139.log`
**Conversation Graph:** `graph_interview_20251213_232141_9krcl2dr_graph_data.json`

---

## Executive Summary

The interview system exhibits a **topic fixation problem** where it repeatedly returns to "enzymes" despite clear signals from the respondent indicating:
1. Lack of knowledge ("I don't really know what enzymes are")
2. Lack of interest ("I am not really curious about them")
3. Confusion about questions ("I do not understand the question")

**Root causes identified:**
- Weak knowledge ceiling penalty (0.1) insufficient to suppress unknown topics
- Fragmented extraction creating isolated nodes that trigger `connect_isolate` cycle
- Missing topic-level exhaustion (only node-level tracking exists)
- Coverage gap persistence despite extensive discussion
- Multiplicative scoring with exponential weights creating unpredictable behavior
- KnowledgeCeilingScorer relies on keyword matching rather than semantic understanding

---

## Part 1: Quantitative Analysis

### Session Statistics
| Metric | Value | Issue |
|--------|-------|-------|
| Total turns | 20 | - |
| Coverage gaps at start | 1 (RTB) | - |
| Coverage gaps at end | 1 (RTB) | Never resolved |
| Total nodes extracted | 53 | High fragmentation |
| Total edges | 33 | 1.6:1 node-to-edge ratio |
| Isolated nodes (final) | 9 (17%) | Many enzyme-related |

### Strategy Distribution
| Strategy | Count | Percentage | Concern |
|----------|-------|------------|---------|
| connect_isolate | 7 | 35% | **Excessive** - drove enzyme fixation |
| deepen_branch | 3 | 15% | Normal |
| clarification | 3 | 15% | High - indicates confusing questions |
| explore_breadth | 3 | 15% | Normal |
| ensure_coverage | 2 | 10% | Should have resolved RTB early |
| resolve_ambiguity | 2 | 10% | Normal |

### Enzyme-Related Nodes (22% of total graph)
12 nodes directly about enzymes were extracted as separate entities, when they should have been consolidated into 2-3 semantic concepts. This fragmentation created isolated nodes that repeatedly triggered `connect_isolate`.

---

## Part 2: Root Cause Analysis

### 2.1 Weak Knowledge Ceiling Detection

**Location:** `arbitration.py:213-299`

**Current Implementation:**
```python
ceiling_patterns = [
    "i don't know", "not sure", "don't understand", ...
]

def _detect_knowledge_ceiling(self, target_label, history):
    for turn in recent_turns:
        response = turn.response.lower()
        target_words = target_label.split()
        target_mentioned = any(word in response for word in target_words if len(word) > 2)
        # ... keyword matching
```

**Problems:**
1. Uses keyword matching instead of semantic understanding
2. Penalty of 0.1 is too weak to suppress topics
3. Target detection relies on word overlap, missing semantic variations
4. Doesn't capture disinterest signals ("not really curious")

---

### 2.2 Multiplicative Scoring Creates Exponential Effects

**Location:** `arbitration.py:1019-1041`

**Current Implementation:**
```python
weighted_score = raw_score ** weight if weight != 1.0 else raw_score
total_score *= weighted_score  # Multiplicative aggregation
```

**Problems:**
1. **Exponential amplification:** `0.2 ** 2.0 = 0.04` crushes to 4%, `2.0 ** 2.0 = 4.0` boosts to 400%
2. **Single penalty dominates:** `[1.0, 1.5, 0.1, 1.2]` produces 0.18 - one bad scorer kills everything
3. **Weight interpretation mismatch:** Documented as "global weight" but acts as exponent
4. **Non-intuitive behavior:** Hard to predict combined effects

---

### 2.3 RedundancyScorer Uses Broken Template Questions

**Location:** `arbitration.py:150-177`

**Current Implementation:**
```python
def _generate_template_question(self, strategy, focus):
    if strategy.id == "connect_isolate":
        return f"how does {label} relate connect"  # Grammatically broken!
    elif strategy.id == "resolve_schema_tension":
        return f"how does {label} lead to"  # Missing target!
```

**Problems:**
1. Grammatically incomplete templates
2. Missing contextual information (targets, elements)
3. No variation by node type
4. Won't match real questions well, reducing deduplication effectiveness

---

### 2.4 BranchHealthScorer Doesn't Distinguish Strategy Shifts

**Location:** `arbitration.py:668-693`

**Problem:** Counts turns without branch growth, but doesn't distinguish:
- Intentional shift to explore different branch
- Extraction failure
- Non-extractable response

**Consequence:** Branch appears "stale" even when shift was intentional and productive elsewhere.

---

### 2.5 Coverage Only Records One Gap Type Per Element

**Location:** `state.py:464-519`

**Current Implementation:**
```python
if reqs.mention:
    if not self.element_node_mappings.get(element_id):
        self.gaps.append(CoverageGap(..., gap_type="unmentioned"))
        continue  # Can't check other requirements if not mentioned
```

**Problem:** The `continue` statement prevents capturing composite gaps. An element can only have ONE gap type, even when multiple requirements are unmet.

---

### 2.6 Focus Count Increments on Selection, Not Success

**Location:** `strategy.py:285`, `state.py:533-542`

**Problem:** Focus count increments when a gap is selected, not after successful extraction.

**Consequence:** If question generation fails or response is non-extractable, focus count still incremented, artificially advancing toward exhaustion.

---

### 2.7 Fragmented Extraction Creates Vicious Cycle

**Cycle Pattern:**
```
LLM extracts enzyme statement as NEW node
         ↓
Node has no edges (isolated)
         ↓
connect_isolate triggers (35% of strategies)
         ↓
System asks about enzymes
         ↓
More enzyme content extracted
         ↓
More isolated enzyme nodes
         ↓
Cycle repeats
```

---

## Part 3: Prompt Quality Review

### 3.1 Extraction Prompt (extraction.py:628-753)

**Strengths:**
- Comprehensive schema guidance with element-specific sentiment rules
- Clear edge extraction priority ("1 edge per 2 nodes")
- Good ambiguity detection criteria
- Element mapping with type-aware sentiment attribution

**Weaknesses:**

1. **No topic clustering guidance:**
   - Prompt says "extract ATOMIC concepts" but doesn't guide on semantic clustering
   - "enzymes make oat milk creamier" and "enzymes just change oats to be creamier" are semantically identical but extracted separately
   - **Recommendation:** Add guidance to check semantic similarity to existing nodes before creating new ones

2. **Missing consolidation instruction:**
   - Prompt: "Use existing nodes when possible"
   - But no explicit instruction to CHECK for semantic overlap
   - **Recommendation:** Add: "Before creating a node, check if an existing node expresses the same concept with different wording"

### 3.2 Question Generation Prompt (generator.py:359-396)

**Strengths:**
- Clear intent and guidance from strategy
- Tactic descriptions available
- Good rules (one question, natural, no leading)

**Weaknesses:**

1. **Momentum guidance only has two branches:**
```python
if momentum.level == "high":
    momentum_guidance = "...probe deeper..."
elif momentum.level == "low":
    momentum_guidance = "...keep simple..."
# Missing: neutral guidance
```
**Recommendation:** Add explicit neutral guidance for balanced engagement.

2. **No awareness of respondent signals:**
   - Prompt doesn't include information about disinterest or confusion signals
   - **Recommendation:** Add context about recent respondent signals (if any negative momentum indicators detected)

3. **No topic exhaustion context:**
   - Prompt doesn't know if the topic has been over-explored
   - **Recommendation:** Add context: "This topic has been discussed in N previous turns"

### 3.3 Momentum Assessment Prompt (extraction.py:551-588)

**Strengths:**
- Detailed high/neutral/low indicators
- Critical balancing rules to avoid misclassification
- Recognizes hedging during reasoning as normal

**Weaknesses:**

1. **No disinterest detection:**
   - Doesn't capture "I'm not curious about this" as a specific signal
   - **Recommendation:** Add disinterest indicator category

2. **No topic-specific momentum:**
   - Assesses overall engagement, not topic-specific interest
   - **Recommendation:** Add guidance to note if disengagement is topic-specific vs general

### 3.4 Extractability Check Prompt (extraction.py:437-463)

**Strengths:**
- Clear criteria for extractable vs non-extractable
- Correctly marks summaries as extractable

**No significant weaknesses identified.**

---

## Part 4: Recommendations (No Keyword Dependencies)

### Priority 1: Replace Keyword-Based Knowledge Ceiling with LLM Assessment

**Current:** Keyword matching in `arbitration.py:228-245`

**Recommended:** Use LLM to assess respondent's knowledge state

**Implementation Concept:**
```python
class LLMKnowledgeCeilingScorer(StrategyScorer):
    """
    Uses LLM to assess if respondent has knowledge about the focus topic.
    Replaces keyword matching with semantic understanding.
    """

    def score(self, strategy, focus, context) -> float:
        if not focus.node and not focus.element:
            return 1.0

        # Get relevant conversation excerpt
        target = focus.node.label if focus.node else focus.element.id
        recent_context = self._get_relevant_turns(target, context.history)

        if not recent_context:
            return 1.0

        # LLM assessment
        assessment = self._assess_knowledge_state(target, recent_context)

        if assessment.state == "no_knowledge":
            return 0.1  # Heavy penalty
        elif assessment.state == "disinterested":
            return 0.15  # Heavy penalty
        elif assessment.state == "uncertain":
            return 0.5  # Moderate caution
        else:
            return 1.0

    def _assess_knowledge_state(self, topic, context) -> KnowledgeAssessment:
        """
        LLM prompt to assess respondent's knowledge state about a topic.

        Returns: KnowledgeAssessment with state and confidence
        States: "knowledgeable", "uncertain", "no_knowledge", "disinterested"
        """
        prompt = f"""Assess the respondent's knowledge state about "{topic}" based on their responses.

Context:
{context}

Assess:
1. Does the respondent have knowledge about this topic?
2. Are they interested in discussing it?
3. Have they expressed confusion or uncertainty?

Return JSON:
{{
  "state": "knowledgeable" | "uncertain" | "no_knowledge" | "disinterested",
  "confidence": 0.0-1.0,
  "evidence": "quote or summary supporting assessment"
}}
"""
        # ... LLM call
```

**Benefits:**
- Semantic understanding vs keyword matching
- Captures nuanced signals (disinterest, confusion, topic-specific uncertainty)
- Adapts to varied phrasings
- Provides evidence for debugging

---

### Priority 2: Add LLM-Based Topic Exhaustion Detection

**Problem:** Current system tracks individual node focus counts, not topic-level exhaustion.

**Implementation Concept:**
```python
class TopicExhaustionScorer(StrategyScorer):
    """
    Uses LLM to detect when a semantic topic cluster has been exhausted.
    """

    def score(self, strategy, focus, context) -> float:
        if not focus.node:
            return 1.0

        # Get all nodes semantically related to focus
        related_nodes = self._get_semantically_related_nodes(
            focus.node, context.graph
        )

        # Check collective visit count
        total_visits = sum(
            context.graph_state.get_node_visit_count(n.id)
            for n in related_nodes
        )

        if total_visits >= 5:  # Topic cluster exhausted
            return 0.2
        elif total_visits >= 3:  # Topic getting stale
            return 0.5

        return 1.0

    def _get_semantically_related_nodes(self, node, graph) -> List[Node]:
        """
        LLM call to identify nodes semantically related to focus node.
        Groups: "enzymes make creamier", "enzymes added", "resulting creamier"
        """
        prompt = f"""Given this node: "{node.label}"

And these other nodes in the graph:
{[n.label for n in graph.nodes.values() if n.id != node.id]}

Which nodes are semantically related (discussing the same underlying topic)?
Return JSON: {{"related_labels": ["label1", "label2", ...]}}
"""
        # ... LLM call with caching
```

---

### Priority 3: Fix Multiplicative Scoring

**Current:** `total_score *= weighted_score ** weight`

**Recommended:** Use additive weighted average with floor

**Implementation:**
```python
def _aggregate_scores(self, scored_results: List[ScorerResult]) -> float:
    """
    Additive weighted average with multiplicative floor for hard penalties.
    """
    # Separate hard penalties (< 0.3) from soft adjustments
    hard_penalties = [r for r in scored_results if r.score < 0.3]
    soft_adjustments = [r for r in scored_results if r.score >= 0.3]

    # If any hard penalty, apply minimum floor
    if hard_penalties:
        worst_penalty = min(r.score for r in hard_penalties)
        floor = worst_penalty  # Hard penalty becomes floor
    else:
        floor = 0.0

    # Weighted average of soft adjustments
    if soft_adjustments:
        total_weight = sum(r.weight for r in soft_adjustments)
        weighted_sum = sum(r.score * r.weight for r in soft_adjustments)
        avg_score = weighted_sum / total_weight if total_weight > 0 else 1.0
    else:
        avg_score = 1.0

    # Apply floor
    return max(floor, avg_score * (1 - floor) + floor)
```

**Benefits:**
- Predictable linear behavior
- Hard penalties still block (floor mechanism)
- Weights work as expected (importance, not exponents)

---

### Priority 4: Fix RedundancyScorer Template Generation

**Current:** Grammatically broken templates

**Recommended:** Use LLM to generate realistic question templates

**Implementation:**
```python
def _generate_template_question(self, strategy, focus) -> str:
    """
    Use LLM to generate a realistic template question for comparison.
    """
    if not focus.node and not focus.element:
        return ""

    target = focus.node.label if focus.node else focus.element.content[:100]

    prompt = f"""Generate a typical interview question that would:
- Use the {strategy.id} strategy
- Focus on: "{target}"

Output only the question, nothing else.
"""

    response = self.llm.complete(
        task=TaskType.TEMPLATE_GENERATION,
        system_prompt="You generate realistic interview questions.",
        user_prompt=prompt,
        temperature=0.3
    )

    return response.content.strip() if response.success else ""
```

---

### Priority 5: Add Semantic Deduplication at Extraction

**Problem:** "enzymes make oat milk creamier" and "enzymes just change oats to be creamier" extracted as separate nodes.

**Recommended Addition to Extraction Prompt:**

Add to `_build_extraction_system_prompt()`:
```
## CRITICAL: Semantic Deduplication

Before creating a NEW node, check if an existing node expresses the same concept:

Existing nodes to check against:
{existing_node_labels}

For each potential new node, ask:
1. Does an existing node express the same underlying concept?
2. Is the new phrasing just a rewording or elaboration?
3. Would a researcher consider these the same data point?

If YES to any: DO NOT create a new node. Instead, note this as supporting evidence
for the existing node (you can add the quote to metadata).

Examples of SAME concept (should NOT create new):
- "enzymes make oat milk creamier" ≈ "resulting product is creamier" (same outcome)
- "creamy texture" ≈ "creamier" (same attribute)

Examples of DIFFERENT concepts (should create new):
- "creamy texture" vs "thick foam" (different attributes)
- "tastes good" vs "mixes well" (different consequences)
```

---

### Priority 6: Fix Coverage to Accept Skeptical Reactions

**Location:** `state.py:471-478`

**Problem:** Coverage requires node mapping, but skepticism ("I don't know what enzymes are") may not create a proper mapping.

**Recommended:** Add explicit skepticism handling in coverage

**Implementation Concept:**
```python
def _recompute_gaps(self, graph: Optional[Graph] = None) -> None:
    """Recompute coverage gaps, including skepticism as valid coverage."""
    self.gaps = []

    for element_id, element in self.reference_elements.items():
        reqs = element.requirements

        # Check mention - include skepticism as valid mention
        if reqs.mention:
            has_mention = bool(self.element_node_mappings.get(element_id))
            has_skepticism = self._has_skepticism_about_element(element_id)

            if not has_mention and not has_skepticism:
                self.gaps.append(CoverageGap(
                    element_id=element_id,
                    gap_type="unmentioned"
                ))
                continue

        # ... rest of gap checking

    def _has_skepticism_about_element(self, element_id: str) -> bool:
        """
        Check if respondent expressed skepticism about this element.
        Uses LLM to assess rather than keyword matching.
        """
        # LLM assessment of whether skepticism was expressed
        # about this specific element in conversation history
        pass
```

---

### Priority 7: Add Constraints to connect_isolate Strategy

**Location:** `interview_logic.yaml` strategy configuration

**Recommended Addition:**
```yaml
strategies:
  connect_isolate:
    intent: >
      Integrate an orphan concept into the broader conversation.
    applies_when: "A node exists with no edges"
    focus: "The isolated node"
    suggested_tactics:
      - relationship_probe
      - temporal_contrast
      - upward_linking
    llm_guidance: >
      Help them see how this fits with what they've already said.
    # NEW: Strategy constraints
    constraints:
      min_node_age_turns: 2          # Node must exist for 2 turns before connecting
      max_consecutive_uses: 2         # Max consecutive connect_isolate
      cooldown_after_max: 3           # Turns to wait after hitting max
      require_plausibility: true      # Must pass plausibility check
      plausibility_threshold: 0.6     # Minimum plausibility score
```

**Implementation in strategy.py:**
```python
def _check_strategy_constraints(self, strategy: Strategy, context) -> bool:
    """Check if strategy constraints are satisfied."""
    constraints = strategy.constraints
    if not constraints:
        return True

    # Check node age
    if constraints.min_node_age_turns:
        if focus.node:
            node_age = context.turn_number - focus.node.created_turn
            if node_age < constraints.min_node_age_turns:
                return False

    # Check consecutive uses
    if constraints.max_consecutive_uses:
        recent_strategies = context.history.get_recent_strategies(
            constraints.max_consecutive_uses
        )
        if all(s == strategy.id for s in recent_strategies):
            return False

    return True
```

---

## Part 5: Implementation Priority Matrix

| Priority | Recommendation | Effort | Impact | Risk |
|----------|---------------|--------|--------|------|
| 1 | LLM-based knowledge ceiling | Medium | High | Low |
| 2 | Topic-level exhaustion detection | Medium | High | Medium |
| 3 | Fix multiplicative scoring | Low | High | Low |
| 4 | Fix RedundancyScorer templates | Medium | Medium | Low |
| 5 | Semantic deduplication prompt | Low | Medium | Low |
| 6 | Coverage skepticism handling | Medium | Medium | Low |
| 7 | connect_isolate constraints | Low | Medium | Low |

---

## Part 6: Additional Logical Weaknesses Found

### 6.1 SchemaTensionReadinessScorer Premature Penalty

**Location:** `arbitration.py:805-871`

**Problem:** `premature_penalty: 0.4` applied before explorations matter. First probe of a node pair gets 40% penalty, discouraging exactly the exploration needed.

**Recommendation:** Scale penalty based on exploration gap:
- If one node unexplored (0 visits): penalty < 0.2
- If both explored once: no penalty
- If one heavily explored, one not: moderate penalty

### 6.2 Missing Extraction Quality Validation Scorer

**Problem:** No scorer checks if recent extractions were productive.

**Recommendation:** Add scorer that penalizes strategies when:
- Previous turn extracted 0 new edges
- Previous extraction had >50% ambiguous nodes
- Recent history shows declining extraction quality

### 6.3 VerticalLadderingScorer BFS Performance

**Location:** `arbitration.py:517-550`

**Problem:** BFS to find terminal nodes has O(V+E) complexity per call, called for every node focus. Large graphs could be expensive.

**Recommendation:** Add depth limit or cache terminal distance calculations.

---

## Appendix: Comparison with Normalized Graph

### Conversation Graph (Current Session)
- **Nodes:** 53
- **Edges:** 33
- **Enzyme nodes:** 12 (22%)
- **Isolated nodes:** 9 (17%)

### Normalized Graph (Example)
- **Nodes:** 13
- **Edges:** 12
- **Enzyme representation:** 1 consolidated node
- **Isolated nodes:** 0

**Key Insight:** The normalized graph consolidates "enzymes make oat milk creamier", "resulting product is creamier", "enzymes added during processing" into a single `process:EnzymaticTreatment` node. This prevents the fragmentation that drives fixation.

---

## Conclusion

The enzyme fixation problem stems from:
1. **Weak semantic understanding** - keyword matching misses nuanced signals
2. **Node-level tracking** - doesn't recognize topic-level exhaustion
3. **Fragmented extraction** - creates isolated nodes triggering connect_isolate
4. **Multiplicative scoring** - creates unpredictable combined effects

The recommended changes use **LLM-based semantic understanding** rather than keyword matching to:
- Detect knowledge gaps and disinterest
- Cluster semantically related nodes
- Generate realistic template questions
- Assess coverage satisfaction

These changes align with the system's LLM-first architecture while eliminating brittle keyword dependencies.
