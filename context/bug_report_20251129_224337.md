# Question Repetition Analysis Report
**Generated:** 2025-11-29 22:43:37 UTC
**Analysis Target:** Sample interview transcript and question generation system

## Executive Summary

Analysis of the sample interview reveals significant question repetition issues, with the interviewer asking essentially the same question about delivery frequency 5 times in different ways. This appears to stem from fundamental design issues in the question generation system rather than simple template overlap.

## The Repetition Problem: Evidence from the Transcript

### Identified Repetitive Questions (5 instances):

**Core Issue:** The interviewer repeatedly asks about delivery frequency preferences, even after the participant has clearly answered:

1. **Turn 2:** "How do you feel about having coffee delivered to you monthly?"
2. **Turn 4:** "What makes weekly delivery work better for you than monthly?" 
3. **Turn 8:** "...what would monthly delivery mean for your coffee routine?"
4. **Turn 10:** "...What is it about that weekly routine that works so well for you?"
5. **Turn 12:** "How often do you think you'd want those deliveries...?"
6. **Turn 14:** "What makes monthly timing feel right for you?"

**Participant's Clear Position:**
- "Month is too long, the beans will go stale"
- "I would prefer weekly delivery"
- "3-4 days" freshness window
- "I buy 2 packs every week"

Despite these clear, consistent answers, the system continued probing the same topic.

## Root Cause Analysis

### 1. **Missing Repetition Detection Mechanism** ‚ùå
**Severity:** CRITICAL

**Issue:** The question generator has NO actual repetition detection or similarity checking logic.

**Evidence:**
- `_question_history` tracks questions but NEVER checks for similarity
- No implementation of the repetition detection configured in `question_templates.yaml` (lines 127-144)
- No similarity algorithms (Jaccard, cosine, etc.) implemented
- The configured `word_overlap_threshold: 0.6` exists only in YAML, not in code

**Code Reality:**
```python
# In generate_question() - NO REPETITION CHECKING
if self.use_llm:
    question = await self._generate_with_llm(opportunity, graph, conversation_history)
    if question:
        question = self._post_process_question(question)
        self._question_history.append(question)  # Just stores, never checks
        return question
```

### 2. **Opportunity Ranker Stuck in Local Maximum** üéØ
**Severity:** HIGH

**Issue:** The opportunity ranking system gets trapped focusing on the same high-priority concept.

**Technical Details:**
- The `weekly_delivery` node has `visit_count: 1` (highest in graph)
- Focus score rewards staying near recently explored nodes (lines 160-182)
- Recency score calculation `1.0 / (visit_count + 1)` doesn't strongly penalize repeated visits
- No "exhaustion" mechanism to retire over-explored concepts

**Focus Stack Problem:**
```python
# update_focus() keeps pushing same node to focus stack
self._focus_stack.append(node_id)  # Weekly delivery gets reinforced
# Results in: ['weekly_delivery', 'weekly_delivery', ...]
```

### 3. **LLM Context Window Too Narrow** ü™ü
**Severity:** MEDIUM

**Issue:** The LLM only sees the last 3 conversation turns, missing the broader context.

**Evidence from transcript:**
- Participant stated preference for weekly delivery multiple times
- But questions keep asking about monthly vs weekly
- LLM prompt template: `conversation_history[-3:]` (line 127-129)

**Missing Context:**
- Initial rejection of monthly delivery
- Consistency of weekly preference across multiple turns
- Evolution to vacuum packaging solution

### 4. **Strategy Determination Logic Flawed** üß†
**Severity:** MEDIUM

**Issue:** The `_determine_strategy()` method keeps selecting `DIG_DEEPER` for the same node.

**Logic Problem:**
```python
def _determine_strategy(self, node_id, node_data):
    if visit_count == 0:
        return QuestionStrategy.INTRODUCE_TOPIC
    if out_degree < 2:  # ‚ùå This keeps triggering for same node
        return QuestionStrategy.DIG_DEEPER
    return QuestionStrategy.CONNECT_CONCEPTS
```

**Result:** A node with low out-degree but high visit count keeps getting `DIG_DEEPER` strategy, leading to repeated probing.

### 5. **Template System Lacks Semantic Variation** üìù
**Severity:** LOW

**Issue:** The dig_deeper templates are semantically similar:
- "You mentioned {node}. Can you tell me more about that?"
- "What specifically about {node} stands out to you?"
- "Help me understand why {node} matters to you."

While grammatically different, they all ask for elaboration on the same concept.

## Deep Technical Analysis

### Opportunity Ranking Algorithm Issues:

**Current Priority Calculation:**
```python
priority = (coverage_score * 3.0) + (depth_score * 1.5) + (recency_score * 1.0) + (focus_score * 2.0)
```

**Problems:**
1. **Focus Weight Too High (2.0):** Keeps interviewer anchored to recent topics
2. **Recency Weight Too Low (1.0):** Doesn't sufficiently penalize repeated visits
3. **No Exhaustion Penalty:** No mechanism to retire over-explored concepts

### Missing Safeguards:

1. **No Question Similarity Check:** Should compare new question to recent ones
2. **No Topic Exhaustion Detection:** Should recognize when participant has nothing new to add
3. **No Strategic Refresh:** Should periodically explore new areas to break loops
4. **No Answer Redundancy Detection:** Should recognize when answers become repetitive

## Hypotheses and Recommendations

### Hypothesis 1: **The System is Designed to Dig, Not Explore**
The opportunity ranker prioritizes depth over breadth, causing it to fixate on partially-explored concepts rather than discovering new ones.

**Recommendation:** Implement exploration vs exploitation trade-off:
```python
# Add exploration bonus for unvisited nodes
exploration_bonus = 1.0 if visit_count == 0 else 0.1
priority = (...) + (exploration_bonus * exploration_weight)
```

### Hypothesis 2: **Focus Mechanism Creates Echo Chambers**
The focus stack reinforces recent topics, creating feedback loops.

**Recommendation:** Implement focus decay:
```python
# Reduce focus score for repeatedly visited nodes
focus_decay = 0.5 ** visit_count  # Exponential decay
focus_score = focus_score * focus_decay
```

### Hypothesis 3: **Context Window Prevents Learning**
The 3-turn context prevents the LLM from seeing the full conversational pattern.

**Recommendation:** Implement semantic context summarization:
```python
# Create semantic summary of key positions established
context_summary = self._create_semantic_summary(conversation_history)
# Include in LLM prompt: "Key positions established: {context_summary}"
```

### Hypothesis 4: **No Semantic Exit Criteria**
The system lacks mechanisms to recognize when a topic is exhausted.

**Recommendation:** Implement topic exhaustion detection:
```python
def is_topic_exhausted(self, node_id, conversation_history):
    # Check for repeated answers, declining elaboration, etc.
    recent_answers = self._extract_recent_answers_about(node_id, conversation_history)
    return self._answers_show_no_new_info(recent_answers)
```

## Immediate Action Items

### Critical Fixes (Implement Immediately):

1. **Add Question Similarity Detection**
```python
def _is_question_similar(self, new_question, history, threshold=0.7):
    for old_question in history[-5:]:  # Check last 5 questions
        similarity = self._calculate_similarity(new_question, old_question)
        if similarity > threshold:
            return True
    return False
```

2. **Implement Topic Exhaustion Detection**
```python
def _is_topic_exhausted(self, node_id, conversation_history):
    # Check if participant has repeatedly given similar answers
    answers_about_topic = self._extract_answers_about(node_id, conversation_history)
    if len(answers_about_topic) >= 3:  # Asked 3+ times
        return self._answers_are_redundant(answers_about_topic)
    return False
```

3. **Reduce Focus Weight**
```python
# In opportunity ranker - reduce focus weight to prevent fixation
self.focus_weight = 0.5  # Down from 2.0
```

4. **Increase Recency Penalty**
```python
def _calculate_recency_score(self, visit_count: int) -> float:
    # Stronger penalty for repeated visits
    return 1.0 / (visit_count + 2)  # Changed from +1 to +2
```

### Medium-Term Improvements:

1. **Implement Semantic Context Window**
2. **Add Strategic Topic Switching**
3. **Create Answer Redundancy Detection**
4. **Implement Exploration Incentives**

## Test Scenarios for Validation

Create test cases that specifically probe repetition issues:

1. **The Broken Record Test:** Participant gives same answer 3 times ‚Üí System should switch topics
2. **The Minimalist Test:** Participant gives short, non-elaborative answers ‚Üí System should explore new areas
3. **The Clear Preference Test:** Participant states clear preference ‚Üí System should not keep asking about alternatives

## Conclusion

The repetition issue is not a simple bug but a fundamental design flaw in the question generation architecture. The system lacks essential safeguards against repetitive questioning and has imbalanced exploration vs. exploitation trade-offs. The fixes require both immediate patches (similarity detection, exhaustion criteria) and architectural improvements (better context management, exploration incentives).

Without these fixes, the system will continue to frustrate participants with repetitive questions, undermining the quality of insights gathered and the overall user experience.

---

**Next Steps:**
1. Implement critical fixes (similarity detection, exhaustion criteria)
2. Re-test with same scenario to validate improvements
3. Create comprehensive test suite for repetition scenarios
4. Monitor for new repetition patterns in future interviews