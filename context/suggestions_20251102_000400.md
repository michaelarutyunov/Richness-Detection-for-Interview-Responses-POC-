# Dynamic Interview Quality Assessment: Metrics Review and Proposals

**Document Purpose:** Technical analysis and recommendations for improving the AI interviewer's dynamic quality assessment system

**Target Audience:** Implementation team (Claude Code)

**Date:** 2025-12-01

---

## Executive Summary

The current richness metric (`sum(node_weights) + sum(edge_boosts)`) measures interview quantity rather than quality. Analysis of two sample interviews reveals that a 7-turn interview (28.0 richness) received higher scores than a 12-turn interview (27.1 richness) despite the latter reaching terminal values and demonstrating deeper laddering. 

**Key Recommendation:** Replace the single richness score with a multi-dimensional quality assessment that evaluates:
1. **Vertical Progress** - Movement up the abstraction hierarchy
2. **Structural Coherence** - Valid graph topology
3. **Interview Efficiency** - Information gain per turn
4. **Chain Completeness** - Connection of attributes to values

This document provides three tiers of metrics: (1) established methods from research, (2) adapted graph theory metrics, and (3) novel proposals for dynamic assessment.

---

## Part 1: Current Richness Metric Analysis

### 1.1 Current Implementation

```yaml
richness_scoring:
  formula: "sum(node_weights) + sum(edge_boosts)"
  thresholds:
    shallow: 0.5
    moderate: 2.0
    rich: 5.0
```

**Node weights:**
- attribute: 0.5
- functional_consequence: 1.0
- psychosocial_consequence: 1.5
- value: 2.0

**Edge boosts:**
- leads_to: 1.0
- blocks: 2.0
- correlates_with: 0.5
- enables: 0.75
- exemplifies: 0.3

### 1.2 Critical Problems Identified

#### Problem 1: Size Dominance Over Structure

**Evidence from Interview 1 (ID: 20251201_225021):**
- 23 nodes, 13 edges → Richness: 27.1
- Edge:Node ratio = 0.57 (sparse connectivity)
- Only 1 value node reached (financial_prudence)

**Evidence from Interview 2 (ID: 20251201_142259):**
- 17 nodes, 14 edges → Richness: 28.0
- Edge:Node ratio = 0.82 (better connectivity)
- 0 value nodes reached (75% schema coverage)

**Problem:** Interview 2 scores higher despite:
- Not reaching terminal values (incomplete means-end chains)
- Being shorter (7 vs 12 turns)
- Missing entire node category

The metric rewards verbosity over depth.

#### Problem 2: No Penalty for Structural Issues

Both interviews contain:
- **Disconnected nodes:** Interview 1 has multiple attributes (select_beans_in_advance, roasted_to_liking, quality_guarantee) with no upward connections
- **Dead end turns:** Turn 4 (Interview 1), Turn 5 (Interview 2) - questions that yielded nothing
- **Invalid edge attempts:** Interview 1, Turn 5 shows validation errors for attribute→attribute edges

**Current metric response:** No change in richness score. These quality issues are invisible.

#### Problem 3: Turn-Level Score Increases Don't Reflect Quality

**Interview 1, Turn 11:**
- Added 6 nodes in single turn
- Richness increased by 6.0
- Result: Participant data dump, not interviewer skill

**Interview 2, Turn 3:**
- Added 3 nodes with 3 connecting edges
- Richness increased by 5.5
- Result: Good laddering captured

**Problem:** Both receive high scores, but Turn 3 (Interview 2) demonstrates better interviewing technique with connected concepts. The metric cannot distinguish between:
- Participant verbosity vs. interviewer-guided depth
- Multiple disconnected concepts vs. coherent chains

#### Problem 4: Thresholds Are Meaningless

Defined thresholds:
```
shallow: 0.5
moderate: 2.0
rich: 5.0
```

Actual richness scores: 27.1 and 28.0

**Gap:** Thresholds designed for early testing don't scale to real interviews. No calibration to actual data distribution.

#### Problem 5: Missing Dynamic Context

Current metric is cumulative only. It cannot answer:
- "Is this interview improving or plateauing?"
- "Should we probe deeper or introduce new topics?"
- "Is this turn productive or a dead end?"

The system needs turn-by-turn velocity and trajectory assessment, not just cumulative totals.

### 1.3 What Current Metric Actually Measures

Formal decomposition:

```
richness = Σ(node_weights) + Σ(edge_boosts)
         = f(interview_length, participant_verbosity, concept_granularity)
```

**Not captured:**
- Graph topology (connectedness, hierarchy, depth)
- Interview quality (probing effectiveness, dead ends)
- Progress toward goals (reaching values)
- Structural validity (cycles, type violations)

---

## Part 2: Established Methods from Research Literature

### 2.1 Means-End Chain Literature (Reynolds & Gutman 1988)

These are **standard practices** in laddering research, suitable for dynamic assessment.

#### 2.1.1 Implication Matrix
**What it is:** Frequency count of direct connections between concept pairs

**Dynamic application:**
```python
# Track during interview
direct_links = {}
for edge in current_graph.edges:
    pair = (edge.source, edge.target)
    direct_links[pair] = direct_links.get(pair, 0) + 1

# Quality indicator
unique_pairs = len(direct_links)
repeated_pairs = sum(1 for count in direct_links.values() if count > 1)
redundancy_ratio = repeated_pairs / unique_pairs if unique_pairs > 0 else 0
```

**Use in dynamic assessment:** High redundancy_ratio suggests interviewer is circling without new insights.

#### 2.1.2 Dominance Score
**What it is:** Number of complete attribute→value chains passing through a concept

**Dynamic application:**
```python
def calculate_dominance(node_id, graph):
    """Count chains from any attribute to any value passing through this node"""
    chains_through_node = 0
    for attr in graph.get_nodes_by_type('attribute'):
        for value in graph.get_nodes_by_type('value'):
            paths = graph.find_all_paths(attr.id, value.id)
            chains_through_node += sum(1 for path in paths if node_id in path)
    return chains_through_node

# For turn-level quality
central_nodes = [n for n in graph.nodes if calculate_dominance(n.id, graph) >= 2]
centrality_index = len(central_nodes) / len(graph.nodes)
```

**Use in dynamic assessment:** Rising centrality_index indicates emerging core concepts. Suggests good probing strategy.

#### 2.1.3 Abstractness Ratio
**What it is:** Distribution of concepts across abstraction levels

**Dynamic application:**
```python
def calculate_level_balance(graph):
    total = len(graph.nodes)
    if total == 0:
        return None
    
    counts = {
        'attribute': len(graph.get_nodes_by_type('attribute')),
        'functional': len(graph.get_nodes_by_type('functional_consequence')),
        'psychosocial': len(graph.get_nodes_by_type('psychosocial_consequence')),
        'value': len(graph.get_nodes_by_type('value'))
    }
    
    ratios = {k: v/total for k, v in counts.items()}
    
    # Expected ratios from literature (adjust based on domain)
    expected = {'attribute': 0.40, 'functional': 0.35, 'psychosocial': 0.20, 'value': 0.05}
    
    # Chi-square distance from expected
    balance_score = sum((ratios[k] - expected[k])**2 for k in expected)
    
    return {
        'ratios': ratios,
        'balance_score': balance_score,  # Lower is better, 0 = perfect match
        'has_values': counts['value'] > 0
    }
```

**Use in dynamic assessment:** 
- High balance_score early → normal exploration
- High balance_score late → failed to ladder upward
- has_values=False after 8+ turns → intervention needed

### 2.2 Qualitative Research Methods (Glaser & Strauss 1967)

#### 2.2.1 Theoretical Saturation
**What it is:** Point where new interviews/probes stop yielding new concepts

**Dynamic application:**
```python
class SaturationTracker:
    def __init__(self, window_size=3):
        self.window_size = window_size
        self.new_nodes_per_turn = []
    
    def update(self, turn_number, new_node_count):
        self.new_nodes_per_turn.append(new_node_count)
    
    def is_saturated(self):
        if len(self.new_nodes_per_turn) < self.window_size:
            return False
        recent = self.new_nodes_per_turn[-self.window_size:]
        return sum(recent) == 0  # No new nodes in last N turns
    
    def saturation_velocity(self):
        """Rate of decline in new concept generation"""
        if len(self.new_nodes_per_turn) < 4:
            return None
        recent = self.new_nodes_per_turn[-4:]
        # Simple linear regression slope
        x = list(range(len(recent)))
        slope = (sum((i - sum(x)/len(x)) * (v - sum(recent)/len(recent)) 
                     for i, v in zip(x, recent)) /
                 sum((i - sum(x)/len(x))**2 for i in x))
        return slope  # Negative = declining productivity
```

**Use in dynamic assessment:**
- `is_saturated() == True` → Time to conclude or switch topics
- `saturation_velocity() < -0.5` → Interview is winding down

**Status:** Established concept, novel implementation for real-time use

### 2.3 Graph Theory Fundamentals (Applicable to Any Graph)

#### 2.3.1 Connected Components
**What it is:** Maximal subgraphs where any node can reach any other node

**Dynamic application:**
```python
def analyze_connectivity(graph):
    components = graph.find_connected_components()  # Use networkx or custom BFS
    
    largest = max(len(c) for c in components) if components else 0
    total_nodes = len(graph.nodes)
    
    return {
        'component_count': len(components),
        'largest_component_size': largest,
        'connectivity_ratio': largest / total_nodes if total_nodes > 0 else 0,
        'has_orphans': any(len(c) == 1 for c in components)
    }
```

**Quality indicators:**
- `component_count > 1` after 5+ turns → disconnected mental model
- `connectivity_ratio < 0.7` → poor integration
- `has_orphans = True` → extraction errors or incomplete probing

**Status:** Standard graph metric, directly applicable

#### 2.3.2 Path Length Statistics
**What it is:** Shortest path distances between node pairs

**Dynamic application:**
```python
def calculate_depth_metrics(graph):
    # Focus on attribute → value paths (the core means-end structure)
    attributes = graph.get_nodes_by_type('attribute')
    values = graph.get_nodes_by_type('value')
    
    if not values:
        return {'max_depth': 0, 'avg_depth': 0, 'has_complete_chains': False}
    
    path_lengths = []
    for attr in attributes:
        for value in values:
            paths = graph.find_all_paths(attr.id, value.id)
            if paths:
                shortest = min(len(p) - 1 for p in paths)  # -1 for edge count
                path_lengths.append(shortest)
    
    return {
        'max_depth': max(path_lengths) if path_lengths else 0,
        'avg_depth': sum(path_lengths) / len(path_lengths) if path_lengths else 0,
        'has_complete_chains': len(path_lengths) > 0,
        'chain_count': len(path_lengths)
    }
```

**Quality indicators:**
- `max_depth >= 3` → Good laddering (attribute → functional → psychosocial → value)
- `avg_depth < 2` late in interview → Shallow probing
- `chain_count / len(attributes) > 0.5` → Most attributes connect to values

**Status:** Standard graph metric, contextually interpreted for means-end chains

#### 2.3.3 DAG Validation
**What it is:** Check for cycles (means-end chains should be acyclic)

**Dynamic application:**
```python
def validate_dag_structure(graph):
    """Detect cycles in directed graph"""
    visited = set()
    recursion_stack = set()
    cycles_found = []
    
    def dfs(node_id, path):
        visited.add(node_id)
        recursion_stack.add(node_id)
        
        for neighbor in graph.get_outgoing_neighbors(node_id):
            if neighbor not in visited:
                if dfs(neighbor, path + [neighbor]):
                    return True
            elif neighbor in recursion_stack:
                # Found cycle
                cycle_start = path.index(neighbor)
                cycles_found.append(path[cycle_start:] + [neighbor])
                return True
        
        recursion_stack.remove(node_id)
        return False
    
    for node in graph.nodes:
        if node.id not in visited:
            dfs(node.id, [node.id])
    
    return {
        'is_dag': len(cycles_found) == 0,
        'cycle_count': len(cycles_found),
        'cycles': cycles_found
    }
```

**Quality indicators:**
- `is_dag = False` → Structural error, likely extraction problem
- Should trigger warning and review of recent extractions

**Status:** Standard graph validation, critical for means-end integrity

---

## Part 3: Novel Proposals for Dynamic Assessment

These are **newly proposed** metrics designed specifically for turn-by-turn interview quality.

### 3.1 Interview Momentum Score

**Concept:** Rate of productive movement up the abstraction hierarchy

**Rationale:** Good interviews maintain upward pressure toward values. Plateaus or downward movement indicate probing issues.

**Implementation:**
```python
class MomentumTracker:
    def __init__(self):
        self.turn_history = []  # List of (turn_num, max_level_reached)
        self.level_map = {'attribute': 0, 'functional_consequence': 1, 
                         'psychosocial_consequence': 2, 'value': 3}
    
    def update(self, turn_num, graph):
        if graph.nodes:
            max_level = max(self.level_map[n.type] for n in graph.nodes)
        else:
            max_level = 0
        self.turn_history.append((turn_num, max_level))
    
    def calculate_momentum(self, window=3):
        """Calculate recent upward movement velocity"""
        if len(self.turn_history) < window:
            return 0.0
        
        recent = self.turn_history[-window:]
        # Linear regression slope
        turns = [t[0] for t in recent]
        levels = [t[1] for t in recent]
        
        n = len(turns)
        mean_turn = sum(turns) / n
        mean_level = sum(levels) / n
        
        numerator = sum((t - mean_turn) * (l - mean_level) for t, l in zip(turns, levels))
        denominator = sum((t - mean_turn) ** 2 for t in turns)
        
        slope = numerator / denominator if denominator > 0 else 0
        
        return {
            'momentum': slope,  # Positive = upward, negative = downward
            'current_level': max_level if self.turn_history else 0,
            'status': self._interpret_momentum(slope)
        }
    
    def _interpret_momentum(self, slope):
        if slope > 0.5:
            return 'rapid_progress'
        elif slope > 0:
            return 'steady_progress'
        elif slope == 0:
            return 'plateau'
        else:
            return 'regression'
```

**Decision logic:**
- `momentum > 0.5` after turn 5 → Interview going well, maintain strategy
- `momentum == 0` after turn 3 → Need deeper probing
- `momentum < 0` → Likely extraction error or participant confusion

**Why it's novel:** Real-time trajectory assessment isn't standard in qualitative research

### 3.2 Probing Efficiency Index

**Concept:** Information gain per interviewer intervention

**Rationale:** Dead end questions waste time. Track which probing strategies yield results.

**Implementation:**
```python
class ProbingEfficiencyTracker:
    def __init__(self):
        self.turn_data = []  # (turn, strategy, nodes_added, edges_added, new_level_reached)
    
    def record_turn(self, turn_num, strategy, nodes_before, edges_before, 
                    max_level_before, graph_after):
        nodes_added = len(graph_after.nodes) - nodes_before
        edges_added = len(graph_after.edges) - edges_before
        
        if graph_after.nodes:
            level_map = {'attribute': 0, 'functional_consequence': 1,
                        'psychosocial_consequence': 2, 'value': 3}
            max_level_after = max(level_map[n.type] for n in graph_after.nodes)
            new_level = max_level_after > max_level_before
        else:
            new_level = False
        
        self.turn_data.append({
            'turn': turn_num,
            'strategy': strategy,
            'nodes_added': nodes_added,
            'edges_added': edges_added,
            'new_level_reached': new_level,
            'is_productive': nodes_added > 0 or edges_added > 0
        })
    
    def calculate_efficiency(self, recent_n=5):
        """Analyze recent probing success rate"""
        if not self.turn_data:
            return None
        
        recent = self.turn_data[-recent_n:] if len(self.turn_data) >= recent_n else self.turn_data
        
        total_turns = len(recent)
        productive_turns = sum(1 for t in recent if t['is_productive'])
        level_advances = sum(1 for t in recent if t['new_level_reached'])
        
        avg_nodes = sum(t['nodes_added'] for t in recent) / total_turns
        avg_edges = sum(t['edges_added'] for t in recent) / total_turns
        
        return {
            'success_rate': productive_turns / total_turns,
            'level_advance_rate': level_advances / total_turns,
            'avg_nodes_per_turn': avg_nodes,
            'avg_edges_per_turn': avg_edges,
            'status': self._interpret_efficiency(productive_turns / total_turns)
        }
    
    def _interpret_efficiency(self, success_rate):
        if success_rate >= 0.8:
            return 'highly_efficient'
        elif success_rate >= 0.6:
            return 'moderate'
        elif success_rate >= 0.4:
            return 'struggling'
        else:
            return 'ineffective'
    
    def strategy_effectiveness(self):
        """Which strategies are working?"""
        strategy_stats = {}
        for turn in self.turn_data:
            s = turn['strategy']
            if s not in strategy_stats:
                strategy_stats[s] = {'attempts': 0, 'successes': 0}
            strategy_stats[s]['attempts'] += 1
            if turn['is_productive']:
                strategy_stats[s]['successes'] += 1
        
        for s, stats in strategy_stats.items():
            stats['success_rate'] = stats['successes'] / stats['attempts']
        
        return strategy_stats
```

**Decision logic:**
- `success_rate < 0.5` → Current probing strategy not working, try different approach
- `level_advance_rate == 0` after 4 turns → Explicitly probe upward ("Why does X matter to you?")
- `strategy_effectiveness()` → Learn which question types work for this participant

**Why it's novel:** Explicit turn-level ROI tracking for interview decisions

### 3.3 Semantic Coherence Score (Embedding-Based)

**Concept:** Are newly added nodes semantically related to existing graph?

**Rationale:** Random jumps suggest participant confusion or poor question flow. Related concepts suggest coherent exploration.

**Implementation:**
```python
import numpy as np
from sentence_transformers import SentenceTransformer

class SemanticCoherenceTracker:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.embeddings = {}  # node_id -> embedding vector
    
    def add_node_embedding(self, node_id, node_label):
        """Store embedding for new node"""
        self.embeddings[node_id] = self.model.encode(node_label)
    
    def calculate_turn_coherence(self, new_node_ids, existing_node_ids):
        """How related are new nodes to existing graph?"""
        if not new_node_ids or not existing_node_ids:
            return None
        
        new_embeds = np.array([self.embeddings[nid] for nid in new_node_ids])
        existing_embeds = np.array([self.embeddings[nid] for nid in existing_node_ids])
        
        # Cosine similarity between each new node and existing nodes
        similarities = []
        for new_emb in new_embeds:
            sims = [self._cosine_similarity(new_emb, ex_emb) for ex_emb in existing_embeds]
            max_sim = max(sims)  # Best match to existing graph
            similarities.append(max_sim)
        
        avg_coherence = np.mean(similarities)
        
        return {
            'coherence_score': avg_coherence,  # 0-1 scale
            'status': self._interpret_coherence(avg_coherence)
        }
    
    def _cosine_similarity(self, a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def _interpret_coherence(self, score):
        if score > 0.7:
            return 'highly_related'
        elif score > 0.5:
            return 'moderately_related'
        elif score > 0.3:
            return 'loosely_related'
        else:
            return 'disconnected'
    
    def detect_redundancy(self, new_node_id, threshold=0.85):
        """Is this new node too similar to existing ones?"""
        if new_node_id not in self.embeddings:
            return None
        
        new_emb = self.embeddings[new_node_id]
        similarities = [
            (node_id, self._cosine_similarity(new_emb, emb))
            for node_id, emb in self.embeddings.items()
            if node_id != new_node_id
        ]
        
        duplicates = [(nid, sim) for nid, sim in similarities if sim > threshold]
        
        return {
            'is_redundant': len(duplicates) > 0,
            'similar_nodes': duplicates
        }
```

**Decision logic:**
- `coherence_score < 0.3` → Participant jumped topics, consider redirect or accept topic switch
- `is_redundant = True` → Don't probe this node further, consolidate with similar node
- Track coherence over time to detect topic drift

**Why it's novel:** Real-time semantic similarity assessment for interview guidance (requires embedding model)

### 3.4 Chain Completion Index

**Concept:** Proportion of attributes that successfully ladder up to values

**Rationale:** The goal is complete means-end chains, not just concept collection

**Implementation:**
```python
def calculate_chain_completion(graph):
    """What percentage of attributes connect to values?"""
    attributes = graph.get_nodes_by_type('attribute')
    values = graph.get_nodes_by_type('value')
    
    if not attributes:
        return {'completion_index': 1.0, 'status': 'no_attributes'}
    
    if not values:
        return {'completion_index': 0.0, 'status': 'no_values_reached'}
    
    connected_attributes = 0
    incomplete_attributes = []
    
    for attr in attributes:
        has_path_to_value = False
        for value in values:
            paths = graph.find_all_paths(attr.id, value.id)
            if paths:
                has_path_to_value = True
                break
        
        if has_path_to_value:
            connected_attributes += 1
        else:
            incomplete_attributes.append(attr.id)
    
    completion_index = connected_attributes / len(attributes)
    
    # Additional analysis
    avg_attribute_depth = 0
    for attr in attributes:
        # Find longest path from this attribute
        max_depth = 0
        for node in graph.nodes:
            paths = graph.find_all_paths(attr.id, node.id)
            if paths:
                max_depth = max(max_depth, max(len(p) - 1 for p in paths))
        avg_attribute_depth += max_depth
    
    avg_attribute_depth /= len(attributes) if attributes else 1
    
    return {
        'completion_index': completion_index,
        'connected_count': connected_attributes,
        'total_attributes': len(attributes),
        'incomplete_attributes': incomplete_attributes,
        'avg_depth': avg_attribute_depth,
        'status': _interpret_completion(completion_index, avg_attribute_depth)
    }

def _interpret_completion(index, avg_depth):
    if index >= 0.7 and avg_depth >= 2.5:
        return 'strong_chains'
    elif index >= 0.5 or avg_depth >= 2.0:
        return 'partial_chains'
    elif index >= 0.3:
        return 'weak_chains'
    else:
        return 'fragmented'
```

**Decision logic:**
- `completion_index < 0.3` after 8 turns → Focus on deepening existing attributes
- `incomplete_attributes` → List of specific nodes to probe
- `status == 'fragmented'` → Interview needs restructuring

**Why it's novel:** Goal-oriented metric specific to means-end methodology

### 3.5 Diversity-Depth Balance Score

**Concept:** Trade-off between exploring new topics (breadth) vs. laddering existing ones (depth)

**Rationale:** Optimal interviews balance coverage and depth

**Implementation:**
```python
class DiversityDepthTracker:
    def __init__(self):
        self.turn_metrics = []  # List of (turn, unique_topics, max_depth)
    
    def update(self, turn_num, graph):
        # Estimate topics via connected components
        components = graph.find_connected_components()
        unique_topics = len(components)
        
        # Calculate max chain depth
        depth_metrics = calculate_depth_metrics(graph)
        max_depth = depth_metrics['max_depth']
        
        self.turn_metrics.append((turn_num, unique_topics, max_depth))
    
    def calculate_balance(self):
        """Current state of breadth vs depth"""
        if not self.turn_metrics:
            return None
        
        _, topics, depth = self.turn_metrics[-1]
        
        # Ideal: 2-3 topics with depth 3-4
        # Adjust these targets based on interview length and domain
        topic_target = 2.5
        depth_target = 3.5
        
        # Normalized scores (0-1, where 1 = at target)
        topic_score = min(topics / topic_target, 2 - topics / topic_target)
        depth_score = min(depth / depth_target, 2 - depth / depth_target)
        
        # Balance: Both should be high
        balance_score = (topic_score + depth_score) / 2
        
        return {
            'current_topics': topics,
            'current_depth': depth,
            'balance_score': balance_score,
            'recommendation': self._recommend_action(topics, depth, topic_target, depth_target)
        }
    
    def _recommend_action(self, topics, depth, topic_target, depth_target):
        if topics < topic_target * 0.7 and depth < depth_target * 0.7:
            return 'introduce_new_topic'
        elif topics < topic_target * 0.7:
            return 'explore_breadth'
        elif depth < depth_target * 0.7:
            return 'probe_deeper'
        else:
            return 'maintain_balance'
    
    def trajectory_analysis(self, window=5):
        """Are we moving toward good balance?"""
        if len(self.turn_metrics) < window:
            return None
        
        recent = self.turn_metrics[-window:]
        
        # Trend in topics
        topic_trend = recent[-1][1] - recent[0][1]
        # Trend in depth
        depth_trend = recent[-1][2] - recent[0][2]
        
        return {
            'topic_trajectory': 'expanding' if topic_trend > 0 else 'stable' if topic_trend == 0 else 'consolidating',
            'depth_trajectory': 'deepening' if depth_trend > 0 else 'stable' if depth_trend == 0 else 'shallowing'
        }
```

**Decision logic:**
- `recommendation == 'probe_deeper'` → Use "why" questions on existing nodes
- `recommendation == 'explore_breadth'` → Introduce seed topics
- `topic_trajectory == 'expanding'` + `depth_trajectory == 'shallowing'` → Warning: surface-level exploration

**Why it's novel:** Dynamic guidance for breadth vs depth navigation

---

## Part 4: Recommended Multi-Dimensional Quality Score

### 4.1 Proposed Composite Score Structure

Replace single `richness` metric with:

```python
class InterviewQualityAssessment:
    """Multi-dimensional quality tracking"""
    
    def __init__(self):
        self.momentum_tracker = MomentumTracker()
        self.efficiency_tracker = ProbingEfficiencyTracker()
        self.saturation_tracker = SaturationTracker()
        # Optional: semantic_tracker = SemanticCoherenceTracker()
    
    def assess_turn(self, turn_num, graph_before, graph_after, strategy):
        """Compute quality metrics after each turn"""
        
        # 1. Structural Quality (Established methods)
        connectivity = analyze_connectivity(graph_after)
        depth = calculate_depth_metrics(graph_after)
        balance = calculate_level_balance(graph_after)
        dag_valid = validate_dag_structure(graph_after)
        
        # 2. Progress Quality (Novel methods)
        self.momentum_tracker.update(turn_num, graph_after)
        momentum = self.momentum_tracker.calculate_momentum()
        
        nodes_before = len(graph_before.nodes)
        edges_before = len(graph_before.edges)
        level_map = {'attribute': 0, 'functional_consequence': 1,
                    'psychosocial_consequence': 2, 'value': 3}
        max_level_before = max([level_map[n.type] for n in graph_before.nodes]) if graph_before.nodes else 0
        
        self.efficiency_tracker.record_turn(
            turn_num, strategy, nodes_before, edges_before, max_level_before, graph_after
        )
        efficiency = self.efficiency_tracker.calculate_efficiency()
        
        new_node_count = len(graph_after.nodes) - nodes_before
        self.saturation_tracker.update(turn_num, new_node_count)
        saturation = self.saturation_tracker.saturation_velocity()
        
        # 3. Completeness Quality (Novel method)
        completion = calculate_chain_completion(graph_after)
        
        # Compile overall assessment
        return {
            'turn': turn_num,
            'structural': {
                'connectivity_ratio': connectivity['connectivity_ratio'],
                'max_depth': depth['max_depth'],
                'chain_count': depth['chain_count'],
                'level_balance': balance['balance_score'],
                'is_valid_dag': dag_valid['is_dag']
            },
            'progress': {
                'momentum': momentum['momentum'] if momentum else 0,
                'efficiency': efficiency['success_rate'] if efficiency else 0,
                'saturation_velocity': saturation if saturation else 0
            },
            'completeness': {
                'chain_completion_index': completion['completion_index'],
                'has_values': balance['ratios'].get('value', 0) > 0
            },
            'overall_status': self._synthesize_status(connectivity, depth, momentum, efficiency, completion)
        }
    
    def _synthesize_status(self, connectivity, depth, momentum, efficiency, completion):
        """High-level interview quality assessment"""
        
        # Red flags
        if connectivity['connectivity_ratio'] < 0.5:
            return 'fragmented_poor'
        if depth['max_depth'] == 0 and completion['has_values'] == False:
            return 'surface_level'
        if efficiency and efficiency['success_rate'] < 0.4:
            return 'inefficient_probing'
        
        # Good signs
        if (depth['max_depth'] >= 3 and 
            completion['completion_index'] > 0.6 and
            connectivity['connectivity_ratio'] > 0.7):
            return 'high_quality'
        
        # In progress
        if momentum and momentum['momentum'] > 0.3:
            return 'progressing_well'
        
        return 'developing'
```

### 4.2 Dimensionality Summary

| Dimension | Metric | Source | Update Frequency |
|-----------|--------|--------|------------------|
| **Structural** | Connectivity ratio | Graph theory (established) | Every turn |
| | Max depth | Graph theory (established) | Every turn |
| | Level balance | Laddering research (established) | Every turn |
| | DAG validity | Graph theory (established) | Every turn |
| **Progress** | Momentum | Novel proposal | Every turn |
| | Efficiency | Novel proposal | Rolling window (3-5 turns) |
| | Saturation | Qualitative research (adapted) | Rolling window (3 turns) |
| **Completeness** | Chain completion | Novel proposal | Every turn |
| | Value coverage | Laddering research (established) | Every turn |

### 4.3 Decision Integration Points

**Turn-level decisions (after each extraction):**
```python
def generate_next_question(graph, quality_assessment):
    """Use quality metrics to guide question generation"""
    
    # Priority 1: Fix structural issues
    if not quality_assessment['structural']['is_valid_dag']:
        return {'strategy': 'review_and_repair', 'priority': 'critical'}
    
    # Priority 2: Respond to efficiency signals
    if quality_assessment['progress']['efficiency'] < 0.4:
        strategy_stats = efficiency_tracker.strategy_effectiveness()
        best_strategy = max(strategy_stats.items(), key=lambda x: x[1]['success_rate'])[0]
        return {'strategy': best_strategy, 'priority': 'high'}
    
    # Priority 3: Address completion gaps
    if quality_assessment['completeness']['chain_completion_index'] < 0.3:
        incomplete = calculate_chain_completion(graph)['incomplete_attributes']
        target_node = random.choice(incomplete)
        return {'strategy': 'probe_upward', 'target': target_node, 'priority': 'medium'}
    
    # Priority 4: Navigate breadth/depth
    balance = diversity_depth_tracker.calculate_balance()
    if balance['recommendation'] == 'probe_deeper':
        # Find highest-level node without value connection
        return {'strategy': 'ladder_upward', 'priority': 'medium'}
    elif balance['recommendation'] == 'explore_breadth':
        return {'strategy': 'introduce_topic', 'priority': 'low'}
    
    # Priority 5: Check for saturation
    if saturation_tracker.is_saturated():
        return {'strategy': 'conclude_interview', 'priority': 'high'}
    
    # Default: Continue current trajectory
    return {'strategy': 'follow_momentum', 'priority': 'low'}
```

**Session-level decisions (interview management):**
```python
def should_continue_interview(turn_num, quality_assessment, max_turns=20):
    """Decide if interview should continue"""
    
    # Hard limit
    if turn_num >= max_turns:
        return False, 'max_turns_reached'
    
    # Quality-based early stopping
    if (quality_assessment['completeness']['chain_completion_index'] > 0.7 and
        quality_assessment['structural']['max_depth'] >= 3 and
        saturation_tracker.is_saturated()):
        return False, 'quality_goals_achieved'
    
    # Efficiency-based early stopping
    if (turn_num > 8 and 
        quality_assessment['progress']['efficiency'] < 0.3):
        return False, 'low_productivity'
    
    # Continue
    return True, 'continue'
```

---

## Part 5: Implementation Roadmap

### Phase 1: Replace Current Richness (Immediate)
**Goal:** Fix critical issues with existing metric

**Changes:**
1. Implement `calculate_depth_metrics()` - depth is missing from current system
2. Implement `calculate_chain_completion()` - goal-oriented metric
3. Implement `analyze_connectivity()` - detect fragmentation
4. Create new composite score:
   ```python
   quality_score = (
       depth_metrics['max_depth'] / 4.0 * 0.3 +          # Depth weight
       completion['completion_index'] * 0.4 +             # Completion weight
       connectivity['connectivity_ratio'] * 0.2 +          # Connectivity weight
       balance['has_values'] * 0.1                        # Value reached weight
   )
   ```

**Why this first:** Uses only graph topology, no new dependencies, directly addresses critique

### Phase 2: Add Progress Tracking (Core Features)
**Goal:** Enable dynamic interview guidance

**Changes:**
1. Implement `MomentumTracker` - track upward movement
2. Implement `ProbingEfficiencyTracker` - learn what works
3. Implement `SaturationTracker` - know when to stop
4. Integrate into question generation logic

**Why this second:** Provides actionable signals for real-time decisions

### Phase 3: Advanced Features (Optional)
**Goal:** Enhanced quality for production

**Changes:**
1. Implement `SemanticCoherenceTracker` (requires sentence-transformers)
2. Implement `DiversityDepthTracker` - breadth/depth guidance
3. Add validation for edge type constraints from schema
4. Build turn-by-turn quality dashboard

**Why this last:** Requires additional dependencies, provides incremental improvements

---

## Part 6: Validation Strategy

### 6.1 Metric Validation Approaches

**For established metrics (connectivity, depth, balance):**
- Compare against manually coded interviews
- Expected patterns:
  - High-quality interviews: connectivity > 0.7, max_depth >= 3, completion > 0.6
  - Poor interviews: connectivity < 0.5, max_depth < 2, completion < 0.3

**For novel metrics (momentum, efficiency):**
- Compare against human interviewer assessments
- A/B testing: Do interviews guided by metrics receive better participant feedback?
- Correlation analysis: Do high-scoring interviews yield more actionable insights?

### 6.2 Calibration Using Existing Data

Use the two sample interviews to set initial thresholds:

**Interview 1 (20251201_225021):**
- Max depth: ~3 (freshly_roasted_beans → fresh_beans → aromatic_coffee → feel_fully_ready_for_day)
- Chain completion: ~4.3% (1 of 23 attributes connects to value)
- Connectivity: Would need computation, estimate ~0.6

**Interview 2 (20251201_142259):**
- Max depth: ~2-3 (freshly_roasted → aromatic_coffee → pleasant_sensation)
- Chain completion: 0% (no values reached)
- Connectivity: estimate ~0.7 (more edges per node)

**Proposed initial thresholds:**
```python
QUALITY_THRESHOLDS = {
    'depth': {'poor': 0, 'fair': 2, 'good': 3, 'excellent': 4},
    'completion': {'poor': 0.0, 'fair': 0.3, 'good': 0.6, 'excellent': 0.8},
    'connectivity': {'poor': 0.4, 'fair': 0.6, 'good': 0.75, 'excellent': 0.85},
    'momentum': {'stalled': 0, 'slow': 0.2, 'good': 0.5, 'rapid': 0.8},
    'efficiency': {'poor': 0.4, 'fair': 0.6, 'good': 0.75, 'excellent': 0.9}
}
```

Refine these after analyzing 10-20 real interviews.

---

## Part 7: Key Takeaways for Implementation

### Critical Points

1. **Current metric is fundamentally flawed**: It measures quantity, not quality. Any improvement requires structural changes, not parameter tuning.

2. **Multi-dimensional assessment is essential**: No single number captures interview quality. Minimum viable set: depth, completion, connectivity.

3. **Dynamic context matters**: Turn-by-turn metrics enable adaptive interviewing. The system should learn from its own performance.

4. **Established methods exist but are limited**: Traditional laddering research provides guidance for post-hoc analysis, not real-time assessment. Adaptation required.

5. **Novel metrics fill gaps**: Momentum, efficiency, and semantic coherence are proposed specifically for dynamic AI interviewing. They're untested but theoretically sound.

6. **Implementation should be phased**: Start with topology-based metrics (Phase 1), add progress tracking (Phase 2), then advanced features (Phase 3).

### Immediate Action Items

**For code implementation team:**

1. Audit current graph data structure - ensure path-finding capabilities exist
2. Implement three core functions:
   - `calculate_depth_metrics(graph)` → dict with max_depth, avg_depth, chain_count
   - `calculate_chain_completion(graph)` → dict with completion_index, incomplete_attributes
   - `analyze_connectivity(graph)` → dict with connectivity_ratio, component_count
3. Replace `richness` score calculation with composite quality score
4. Update thresholds based on real interview distributions
5. Instrument question generation to log: strategy used, nodes/edges added, turn productivity

**For testing:**
- Manually assess 5-10 interviews for "quality" (researcher judgment)
- Check if proposed metrics correlate with human assessment
- Identify edge cases where metrics fail

**For future consideration:**
- Participant experience metrics (confusion signals, engagement)
- Cross-interview consensus building
- Automated consolidation of semantically similar nodes

---

## Appendix: Code Structure Recommendations

### Suggested Module Organization

```
quality_assessment/
├── __init__.py
├── established_metrics.py       # Part 2 - Research-based metrics
│   ├── calculate_depth_metrics()
│   ├── calculate_level_balance()
│   ├── analyze_connectivity()
│   └── validate_dag_structure()
├── dynamic_trackers.py          # Part 3 - Novel proposals
│   ├── MomentumTracker
│   ├── ProbingEfficiencyTracker
│   ├── SaturationTracker
│   ├── SemanticCoherenceTracker (optional)
│   └── DiversityDepthTracker
├── composite_assessment.py      # Part 4 - Integration
│   ├── InterviewQualityAssessment
│   └── quality_based_decision_logic()
└── config.py                    # Thresholds and parameters
    └── QUALITY_THRESHOLDS
```

### Integration Points in Existing System

**After extraction (every turn):**
```python
# Current: richness += new_node_weights + new_edge_boosts
# Proposed:
quality_assessment.assess_turn(turn_num, graph_before, graph_after, strategy)
```

**Before question generation:**
```python
# Current: select strategy based on richness thresholds
# Proposed:
decision = generate_next_question(graph, quality_assessment.get_current_state())
strategy = decision['strategy']
priority = decision['priority']
```

**At interview conclusion:**
```python
# Current: final_richness_score
# Proposed:
final_report = {
    'session_id': session_id,
    'turns': turn_count,
    'quality_metrics': {
        'depth': quality_assessment.get_final_depth(),
        'completion': quality_assessment.get_final_completion(),
        'connectivity': quality_assessment.get_final_connectivity(),
        'efficiency': quality_assessment.get_final_efficiency()
    },
    'overall_assessment': quality_assessment.get_overall_status(),
    'graph_summary': {
        'nodes': len(graph.nodes),
        'edges': len(graph.edges),
        'values_reached': len([n for n in graph.nodes if n.type == 'value'])
    }
}
```

---

## References

**Established Methods:**
- Reynolds, T. J., & Gutman, J. (1988). Laddering theory, method, analysis, and interpretation. Journal of Advertising Research, 28(1), 11-31.
- Glaser, B. G., & Strauss, A. L. (1967). The discovery of grounded theory: Strategies for qualitative research. Aldine.

**Graph Theory Foundations:**
- Cormen, T. H., et al. (2009). Introduction to Algorithms, 3rd Edition. MIT Press.
- NetworkX documentation for practical graph algorithms

**Means-End Chain Applications:**
- Gengler, C. E., & Reynolds, T. J. (1995). Consumer understanding and advertising strategy: Analysis and strategic translation of laddering data. Journal of Advertising Research, 35(4), 19-33.

**Note on Novel Proposals:**
All metrics in Part 3 (Momentum, Probing Efficiency, Semantic Coherence, Chain Completion Index, Diversity-Depth Balance) are original proposals for this document and lack published validation. They represent theoretically motivated extensions of established concepts to the dynamic interview setting.

---

**Document End**

*This analysis provides both theoretical foundations and practical implementation guidance for improving interview quality assessment. Prioritize Phase 1 changes for immediate impact, then progressively add dynamic features as the system matures.*