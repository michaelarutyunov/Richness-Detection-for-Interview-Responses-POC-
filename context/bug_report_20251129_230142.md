# Question Repetition & Relationship Extraction Analysis Report
**Generated:** 2025-11-29 23:01:42 UTC  
**Analysis Target:** Sample interview transcript and graph extraction system

## Executive Summary

Analysis of the sample interview reveals two major systemic issues: **(1) Severe question repetition** with the same topic asked 6 different ways, and **(2) Dramatic under-extraction of relationships** with only 1 edge found among 13 concepts. Both issues stem from fundamental design flaws in the system's core algorithms.

## Issue 1: Question Repetition Problem

### Evidence from Transcript

The interviewer asked about delivery frequency **6 times in different ways** despite clear, consistent answers:

1. **Turn 2:** "How do you feel about having coffee delivered to you monthly?"
2. **Turn 4:** "What makes weekly delivery work better for you than monthly?" 
3. **Turn 8:** "...what would monthly delivery mean for your coffee routine?"
4. **Turn 10:** "...What is it about that weekly routine that works so well for you?"
5. **Turn 12:** "How often do you think you'd want those deliveries...?"
6. **Turn 14:** "What makes monthly timing feel right for you?"

**Participant's Clear Position (stated repeatedly):**
- "Month is too long, the beans will go stale"
- "I would prefer weekly delivery" 
- "3-4 days" freshness window
- "I buy 2 packs every week"

## Issue 2: Relationship Under-Extraction Problem

### The Numbers Tell the Story

**Extracted Concepts (13 nodes):**
- 10 attributes: `premium_subscription`, `freshly_roasted_beans`, `local_coffee`, `aromatic`, `weekly_delivery`, `stale_beans`, `aroma_loss`, `weekly_shopping`, `freshness_of_beans`, `vacuum_packaging`, `bean_freshness`, `purchase_frequency`
- 1 functional consequence: `monthly_delivery`
- 2 additional attributes from later discussion

**Extracted Relationships (1 edge):**
- `vacuum_packaging` â†’ `monthly_delivery` ("if it is a vacuum pack, then monthly should work")

**Result:** 13:1 node-to-edge ratio indicates severe relationship extraction failure

## Root Cause Analysis: Question Repetition

### 1. **Missing Repetition Detection Mechanism** âŒ
**Severity:** CRITICAL

**Issue:** The question generator has NO actual repetition detection or similarity checking logic.

**Evidence:**
- `_question_history` tracks questions but NEVER checks for similarity
- No implementation of the repetition detection configured in `question_templates.yaml` (lines 127-144)
- No similarity algorithms (Jaccard, cosine, etc.) implemented
- The configured `word_overlap_threshold: 0.6` exists only in YAML, not in code

**Code Reality:**
```python
# In generate_question() - NO REPETITION CHECKING
if self.use_llm:
    question = await self._generate_with_llm(opportunity, graph, conversation_history)
    if question:
        question = self._post_process_question(question)
        self._question_history.append(question)  # Just stores, never checks
        return question
```

### 2. **Opportunity Ranker Stuck in Local Maximum** ðŸŽ¯
**Severity:** HIGH

**Issue:** The opportunity ranking system gets trapped focusing on the same high-priority concept.

**Technical Details:**
- The `weekly_delivery` node has `visit_count: 1` (highest in graph)
- Focus score rewards staying near recently explored nodes (lines 160-182)
- Recency score calculation `1.0 / (visit_count + 1)` doesn't strongly penalize repeated visits
- No "exhaustion" mechanism to retire over-explored concepts

## Root Cause Analysis: Relationship Under-Extraction

### 1. **Schema Constraints: The Gatekeeper Effect** ðŸ”

**The Core Issue:** Strict validation rules only allow upward relationships in the means-end chain:

```yaml
edge_types:
  - name: "leads_to"
    valid_sources: ["attribute", "functional_consequence", "psychosocial_consequence"]  
    valid_targets: ["functional_consequence", "psychosocial_consequence", "value"]
    
  - name: "blocks"
    valid_sources: ["attribute", "functional_consequence"]
    valid_targets: ["psychosocial_consequence", "value"]
```

**The Problem:** Most extracted nodes are **attributes** (10/13), but the schema expects relationships to flow **upward**: `attribute` â†’ `functional_consequence` â†’ `psychosocial_consequence` â†’ `value`

**Result:** Same-level connections (attributeâ†’attribute) and insufficient higher-level concepts limit valid edge creation.

### 2. **Conversational Depth: Staying Surface-Level** ðŸŒŠ

**Evidence from Transcript:**
- Questions focused on practical aspects (delivery frequency, freshness, packaging)
- Participant responded with concrete answers rather than abstract insights
- Conversation stayed at attribute level rather than exploring emotional/social meanings
- Only 1 functional consequence (`monthly_delivery`) vs 10 attributes extracted

### 3. **Conservative Extraction Philosophy** ðŸŽ£

**LLM Prompt Instruction:**
```
"Create edges only when relationships are clearly stated or logically necessary"
"Ensure source and target match valid types for the edge"
```

**Missed Valid Relationships:**
- `locally_roasted` â†’ `leads_to` â†’ `fresh` ("locally roasted. it mean the beans will be fresh")
- `monthly_delivery` â†’ `blocks` â†’ `freshness` ("Month is too long, the beans will go stale")
- `weekly_delivery` â†’ `leads_to` â†’ `freshness` ("weekly delivery work better... beans will not go stale")

## Systemic Design Issues

### 1. **Separation of Concerns Failure**
Both issues stem from the same architectural pattern: sophisticated configuration in YAML files but **missing implementation in the actual code**.

- **Repetition Detection:** Configured but not implemented
- **Relationship Inference:** Schema defined but extraction too conservative
- **Quality Assurance:** Guidelines exist but no enforcement mechanisms

### 2. **Optimization vs. Experience Trade-off Failure**

The system prioritizes **natural conversation flow** over **data collection efficiency**, leading to:
- âœ… Natural-feeling interviews
- âŒ Frustrating participant experience (repetition)
- âŒ Suboptimal data quality (sparse relationships)

### 3. **Feedback Loop Missing**

**No mechanisms to learn from extraction failures:**
- No tracking of "almost relationships" for pattern recognition
- No adjustment of extraction confidence based on conversation depth
- No adaptation of questioning strategy based on relationship yield

## Technical Recommendations

### Immediate Fixes (Critical Priority)

1. **Add Question Similarity Detection**
```python
def _is_question_similar(self, new_question, history, threshold=0.7):
    for old_question in history[-5:]:
        similarity = self._calculate_similarity(new_question, old_question)
        if similarity > threshold:
            return True
    return False
```

2. **Implement Topic Exhaustion Detection**
```python
def _is_topic_exhausted(self, node_id, conversation_history):
    answers_about_topic = self._extract_answers_about(node_id, conversation_history)
    if len(answers_about_topic) >= 3:
        return self._answers_are_redundant(answers_about_topic)
    return False
```

3. **Enhance Relationship Extraction**
```python
def _extract_relationships_with_confidence(self, response, existing_nodes):
    # Extract both explicit and implicit relationships
    explicit_edges = self._extract_explicit_relationships(response)
    implicit_edges = self._extract_implicit_relationships(response, confidence_threshold=0.7)
    return explicit_edges + implicit_edges
```

### Medium-term Improvements (High Priority)

1. **Schema Flexibility Enhancement**
- Add same-level relationship types for intermediate connections
- Implement confidence-based relationship validation
- Create progressive relationship extraction (explicit â†’ implicit â†’ inferred)

2. **Question Strategy Optimization**
- Target questions that elicit higher-level concepts when attribute density > 80%
- Implement exploration vs. exploitation trade-offs for relationship discovery
- Add relationship-yield tracking to guide questioning strategy

3. **Context Window Expansion**
- Move from 3-turn to semantic context summarization
- Include relationship history in LLM prompts
- Track conversational themes across multiple turns

## Impact Assessment

### Current State Impact:
- **Participant Frustration:** 6 repetitive questions on same topic
- **Data Quality Issues:** Sparse relationship network (13:1 ratio)
- **Interview Efficiency:** Suboptimal insight depth per time spent
- **System Credibility:** Appears "stuck" or "not listening"

### Post-Fix Expected Improvement:
- **Participant Experience:** Natural conversation flow without repetition
- **Data Quality:** Rich relationship networks revealing mental model structure
- **Interview Efficiency:** Faster achievement of insight depth goals
- **System Intelligence:** Adaptive questioning that responds to participant input

## Conclusion: Fundamental Architecture Issues

Both the repetition problem and relationship under-extraction reveal the same core issue: **sophisticated configuration without corresponding implementation**. The system has excellent design documentation but lacks the critical algorithms that would make it function intelligently.

These are not minor bugs but **architectural deficiencies** that undermine the entire interview experience and data quality. Immediate remediation is essential before the system can be considered viable for research purposes.

---

**Next Actions:**
1. Implement critical fixes for repetition detection and relationship extraction
2. Re-test with same interview scenario to validate improvements
3. Create comprehensive test suite for both issues
4. Monitor for new patterns of failure in future interviews

**Estimated Implementation Time:** 2-3 weeks for critical fixes, 1-2 months for complete solution