# LLM Configuration - Optimized Architecture
# Single configuration file using the new three-section standard
# Provider-agnostic extraction specs with easy provider switching

# ============================================================================
# SECTION 1: MODEL SELECTION - Easy to change
# ============================================================================
graph_extraction_model: "kimi"      # Options: kimi, anthropic, openai, deepseek
question_generation_model: "anthropic" # Options: kimi, anthropic, openai, deepseek

# ============================================================================
# SECTION 2: EXTRACTION SPECS - Provider-agnostic parameters
# ============================================================================
extraction_specs:
  graph_extraction:
    temperature: 0.3      # Consistent, focused extraction
    max_tokens: 1000      # Sufficient for structured data
    timeout_seconds: 15   # Fast extraction
    
  question_generation:
    temperature: 0.7      # Balanced creativity for questions
    max_tokens: 300       # Typical question length
    timeout_seconds: 20   # Adequate for generation

# ============================================================================
# SECTION 3: PROVIDER SPECS - Provider-specific settings only
# ============================================================================
providers:
  kimi:
    api_key_env: "KIMI_API_KEY"     # References .env variable
    base_url: "https://api.moonshot.ai/v1"
    request_timeout: 30
    models:
      graph_extraction: "kimi-k2-turbo-preview"
      question_generation: "kimi-k2-turbo-preview"

  anthropic:
    api_key_env: "ANTHROPIC_API_KEY"
    base_url: null  # Use default
    request_timeout: 45
    models:
      graph_extraction: "claude-haiku-4-5-20251001"
      question_generation: "claude-sonnet-4-5"

  openai:
    api_key_env: "OPENAI_API_KEY"
    base_url: null  # Use default
    organization: null
    request_timeout: 30
    models:
      graph_extraction: "gpt-4o-mini"
      question_generation: "gpt-4o"

  deepseek:
    api_key_env: "DEEPSEEK_API_KEY"
    base_url: "https://api.deepseek.com/v1"
    request_timeout: 30
    models:
      graph_extraction: "deepseek-chat"
      question_generation: "deepseek-chat"

# ============================================================================
# SECTION 4: GLOBAL SETTINGS - Optional overrides
# ============================================================================
retry_config:
  max_retries: 2
  initial_delay_seconds: 1
  backoff_multiplier: 2
  max_delay_seconds: 30

rate_limits:
  requests_per_minute: 60
  tokens_per_minute: 100000